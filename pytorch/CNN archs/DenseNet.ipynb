{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_f , out_f , dropRate = 0.0):\n",
    "\n",
    "        super(BasicBlock , self).__init__()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_f)\n",
    "        # inplace ==> if false it creates a new tensor and performs batch nrom on that if True it performs batch nomr the given tensor\n",
    "        self.a1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_f , out_channels=out_f , kernel_size=3 , stride=1 , padding=1 , bias=False)\n",
    "        self.droprate = dropRate\n",
    "\n",
    "\n",
    "    def forward(self , x):\n",
    "\n",
    "        out = self.conv1(self.a1(self.bn1(x)))\n",
    "\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out , p=self.droprate , training=self.training)\n",
    "\n",
    "        return torch.cat([x , out] , 1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "\n",
    "    def __init__(self , in_f , out_f , dropRate = 0.0):\n",
    "        super(BottleNeck , self).__init__()\n",
    "        inter_f = out_f * 4\n",
    "        self.bn1 = nn.BatchNorm2d(in_f)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_f, inter_f, kernel_size=1, stride=1,\n",
    "                               padding=0, bias=False)\n",
    "        \n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(inter_f)\n",
    "        self.conv2 = nn.Conv2d(inter_f, out_f, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "\n",
    "    def forward(self , x):\n",
    "\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n",
    "\n",
    "        out = self.conv2(self.relu(self.bn2(out)))\n",
    "\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n",
    "\n",
    "        return torch.cat([x , out] , 1)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self , in_f , out_f , dropRate=0.0):\n",
    "\n",
    "        super(TransitionBlock  , self).__init__()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(in_f)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_f , out_f , kernel_size=1 , stride=1 , padding=0 , bias=False)\n",
    "        self.droprate = dropRate\n",
    "\n",
    "\n",
    "    def forward(self , x):\n",
    "\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "\n",
    "        if self.droprate > 0:\n",
    "\n",
    "            out = F.dropout(out , p=self.droprate , inplace=False , training=self.training)\n",
    "\n",
    "        return F.avg_pool2d(out , 2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_layers , in_f , growth_rate , block , dropRate = 0.0):\n",
    "        super(DenseBlock , self).__init__()\n",
    "        self.layer = self._make_layer(block , in_f , growth_rate , nb_layers , dropRate)\n",
    "\n",
    "    def _make_layer(self , block , in_f , growth_rate , nb_layers , dropRate):\n",
    "        layers = []\n",
    "\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(in_f + i*growth_rate , growth_rate , dropRate))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self , x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "\n",
    "    \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "\n",
    "    def __init__(self , depth , num_classes , growth_rate = 12 , reduction = 0.5 , bottleneck = True , dropRate = 0.0):\n",
    "\n",
    "        super(DenseNet , self).__init__()\n",
    "\n",
    "        in_f = 2 * growth_rate\n",
    "        n = (depth - 4)/3\n",
    "\n",
    "        if bottleneck == True:\n",
    "            n = n/2\n",
    "            block = BottleNeck\n",
    "\n",
    "        else:\n",
    "            block = BasicBlock\n",
    "\n",
    "        n = int(n)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d( n ,in_f , growth_rate , block , dropRate)\n",
    "        \n",
    "        # 1st block\n",
    "        self.block1 = DenseBlock(n , in_f , growth_rate , block , dropRate)\n",
    "        in_f = int(in_f + n * growth_rate)\n",
    "        self.trans1 = TransitionBlock(in_f , int(math.floor(in_f * reduction)) , dropRate=dropRate)\n",
    "        in_f = int(math.floor(in_f*reduction))\n",
    "\n",
    "        # 2nd block\n",
    "        self.block2 = DenseBlock(n , in_f , growth_rate , block , dropRate)\n",
    "        in_f = int(in_f + n*growth_rate)\n",
    "        self.trans2 = TransitionBlock(in_f , int(math.floor(in_f *reduction )) , dropRate=dropRate)\n",
    "        in_f = int(math.floor(in_f * reduction))\n",
    "\n",
    "        # 3rd block\n",
    "\n",
    "        self.block3 = DenseBlock(n , in_f , growth_rate , block , dropRate)\n",
    "        in_f = int(in_f + n*growth_rate)\n",
    "\n",
    "        # gloabal average pooling\n",
    "        self.bn1 = nn.BatchNorm2d(in_f)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(in_f , num_classes)\n",
    "        self.in_f = in_f\n",
    "\n",
    "        # xavier initialization\n",
    "        for m in self.modules():\n",
    "\n",
    "            if isinstance(m , nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0 , math.sqrt(2. / n))\n",
    "\n",
    "            elif isinstance( m  , nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "            elif isinstance(m , nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self ,x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.block1(out))\n",
    "        out = self.trans2(self.block2(out))\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.in_planes)\n",
    "        return self.fc(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
